{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 12\n",
    "## Introduction to GPUs (Graphics Processing Units)\n",
    "### Apr. 19, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of this lecture is based on the previous materials, see:\n",
    "\n",
    "https://nyu-cds.github.io/python-gpu/\n",
    "\n",
    "https://nyu-cds.github.io/python-numba/05-cuda/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "A central processing unit (CPU) is designed to handle complex tasks, emulating virtual machines, control complex flows and branching, security, etc. In contrast, graphical processing units (GPUs) only do one thing well, namely, to handle billions of repetitive low level tasks.\n",
    "\n",
    "---\n",
    "\n",
    "Originally designed for the **rendering of triangles** in 3D graphics:\n",
    "[https://en.wikipedia.org/wiki/Triangle_mesh](https://en.wikipedia.org/wiki/Triangle_mesh)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**GPU**s have 1000s of **arithmetic logic units** (ALUs) compared with traditional CPUs that commonly have only 4 or 8. \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Many types of scientific algorithms spend most of their time doing just what GPUs are good for: performing billions of repetitive arithmetic operations.\n",
    "\n",
    "\n",
    "<img src=\"cpugpuarch.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "The following diagram shows how GPU performance has increased compared to traditional CPU architetures along the years.\n",
    "\n",
    "<img src=\"01-flops.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Difference between a CPU and a GPU\n",
    "This [video](https://www.youtube.com/watch?v=-P28LKWTzrI) is a funny illustration of the difference, in terms of processing capability, between CPUs and GPUs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- major topics today are on gpu's \n",
    "- if you want to speed up lin algebra etc kuda can be usefull \n",
    "- open cl is general purpose dont need nvidia\n",
    "- arhcitecture of GPU's\n",
    "    - there is a difrence between cpu and gpu\n",
    "    - cpu is common, the idea is there are a few cores (or one) have some cache and units that can do all the heavy lifting computations \n",
    "    - GPU does really heavy duty computation but can not do a lot of parralization \n",
    "    - gpu are designed for doing many light weight operations in parrallel\n",
    "    - if you want to do pde on a big domain if you can break your domian down into simple areas where you can do simple operations, \n",
    "    - you can use the cpu and achive final results much faster \n",
    "    - some historcial background gpu were designed for rending stuff\n",
    "    - triangle mesh if you want to render computer graphics have to use triangles and stuff\n",
    "    - before kuda all the programing stuff has to be done using customized language like this \n",
    "    - similar to this we can write pyhton, but used to have to write lower level code. \n",
    "    - gpu is a lot of little cores, that can be parallelized suoer well\n",
    "    - gpu have spead up a lot since 2012 or so that is when al the deep net stuff took off\n",
    "    - single perceision double perscision etc. \n",
    "    - for cpu single percision is still faster, but realivly slow\n",
    "    - for cpu the limit is defined cores\n",
    "    - for gpus there are also core numbers there is memory size, so there are still hardware limitations. \n",
    "    - threads still do physical operaitons, but can buy multiple gpus and run thing in parallele\n",
    "    - it makes sense to alos compute the number of flops per dollar. \n",
    "    - the dollar per g flop since, gpus are cheaper than cpu. if you look at it from an economical sense. \n",
    "- cpu is good at sequential actions \n",
    "- gpu does many things in parallel. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "\n",
    "When computer scientists first attempted to use GPUs for scientific computing, the scientific codes had to be mapped onto operations designed to render triangles. This was incredibly difficult to do, and took a lot of time and dedication. \n",
    "\n",
    "---\n",
    "\n",
    "Nowadays, there are **high level languages** (such as **CUDA** and **OpenCL**) that target the GPUs directly, so GPU programming is rapidly becoming mainstream in the scientific community.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\"OpenCL is an open standard maintained by the non-profit technology consortium Khronos Group. Conformant implementations are available from Altera, AMD, Apple (OpenCL along with OpenGL is deprecated for Apple hardware, in favor of Metal), ARM, Creative, IBM, Imagination, Intel, Nvidia, Qualcomm, Samsung, Vivante, Xilinx, and ZiiLABS.\"\n",
    "\n",
    "CUDA is only implemented by Nvidia. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A **GPU program** comprises two parts: \n",
    "1. a *host part* that runs on the CPU: sets up the **parameters** and **data** for the computation\n",
    "2. one or more *kernels* that run on the GPU: perform the **actual computation**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- before modern languages it was really hard to use gpus for programing, nowadays it is a bit easier since you can use CUDA nad openCL\n",
    "- think of cuda and open cl as highe level code to run sutff on gpu's\n",
    "- cuda is defined to run with nvidia \n",
    "- open cl is more general\n",
    "- cuda is likely more optemized \n",
    "- numba has a simulator, that lets you run cuda code in a virtual enviorment \n",
    "gpu program\n",
    "- still requires a cpu that schedules thing. \n",
    "- this is the same for gpu programs, controls parameters memory manamgnet etc\n",
    "- that also has soemthing to do with the io comunicaitons \n",
    "- if you want to optimzie gpu progmras shoudl sitll miimze omcunation do alall the compucations al tonce and send them bak \n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "\n",
    "## CUDA Programming\n",
    "\n",
    "The CUDA parallel programming model has **three key abstractions** at its core:\n",
    "- a hierarchy of thread groups\n",
    "- shared memories\n",
    "- barrier synchronization\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- will only really look at the first 2 of these. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "**Granularity** in parallel programming: amount of computation vs communication.\n",
    "* **Fine-grained**: individual tasks are relatively small in terms of code size and execution time. The data is transferred among processors frequently in amounts of one or a few memory words.\n",
    "* **Coarse-grained**: data is communicated infrequently, after larger amounts of computation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "graunlairty thhere is fine and course depending on how often want to comuante data, there is a trade off \n",
    "- would often prefer course, unless the complex is more complex. \n",
    "\n",
    "- there is the idea of a kernel\n",
    "- all this is virutal \n",
    "0 each kerneal has multiple blcoks each has mutliple threads\n",
    "- can change number of threads and blcoks you have "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The CUDA abstractions:\n",
    "* fine-grained data parallelism and thread parallelism (thread blocks)\n",
    "* coarse-grained data parallelism and task parallelism (grid)\n",
    "\n",
    "They guide the programmer to partition the problem into coarse sub-problems that can be solved independently in parallel by blocks of threads, and each sub-problem into finer pieces that can be solved cooperatively in parallel by all threads within the block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "- A kernel is executed in parallel by an array of threads:\n",
    "    - All threads run the same code.\n",
    "    - Each thread has an ID that it uses to compute memory addresses and make control decisions.\n",
    "\n",
    "- Threads are arranged as a grid of thread blocks:\n",
    "    - Different grid/block can have different kernels  \n",
    "    - Threads from the same block have access to a shared memory and their execution can be synchronized\n",
    "\n",
    "<img src=\"threadgrid.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Thread blocks are required to execute independently: \n",
    "    - It must be possible to execute them in any order, in parallel or in series \n",
    "    - Threads within a block can cooperate by sharing data through some shared memory and by synchronizing their execution to coordinate memory accesses.\n",
    "    - The grid of blocks and the thread blocks can be 1, 2, or 3-dimensional.\n",
    "\n",
    "<img src=\"threadmapping.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "CUDA is designed for a specific GPU architecture, namely NVIDIA’s Streaming Multiprocessors (SM). \n",
    "- Each SM has:\n",
    "    - a set of execution units\n",
    "    - a set of registers \n",
    "    - a chunk of shared memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"sm.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "In an NVIDIA GPU, the basic unit of execution is the __warp__. A warp is a collection of threads, 32 in current implementations, that are executed simultaneously by an SM. Multiple warps can be executed on an SM at once.\n",
    "\n",
    "When a CUDA program on the host CPU invokes a kernel grid, the blocks of the grid are enumerated and distributed to SMs with available execution capacity. The threads of a thread block execute concurrently on one SM, and multiple thread blocks can execute concurrently on one SM. As thread blocks terminate, new blocks are launched on the vacated SMs.\n",
    "\n",
    "The mapping between warps and thread blocks can affect the performance of the kernel. It is usually a good idea to keep the size of a thread block a multiple of 32 in order to avoid this as much as possible.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- used to have physical arichterecture \n",
    "- each of the processssors has some number of cores. \n",
    "- this architecture has some fixed nuumber of cpu cores. \n",
    "- keep going into sucessor for the lates info \n",
    "- eahc sm has 128 cores that are called th estreaming multiplprocessors. \n",
    "- each block is a warp a warp is a colelction of threads (32) \n",
    "- when ever writing programs make sure number of threads is multiple of 32\n",
    "- the idea is that you should keep these numbers in mind, to optimzine the mapping \n",
    "- the take home is that warp is the most bassic unit of execution \n",
    "- kuda is for nvida there is a notion of sm \n",
    "- eahc sm has a uniot of exation uints \n",
    "- and a set of registers that register shared functions \n",
    "- the idea is that theree is some local indexing, if you want to get some floabl one, you can have a 3d block\n",
    "- the idea is that if you want hte gloabl thread idea, need the second one by he dimnsion of dx. \n",
    "- need to go over all row to get to second row. \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Thread Identity\n",
    "The index of a thread and its thread ID relate to each other as follows:\n",
    "- For a 1-dimensional block, the thread index and thread ID are the same\n",
    "- For a 2-dimensional block, the thread index (x,y) has thread ID=x+yDx, for block size (Dx,Dy)\n",
    "- For a 3-dimensional block, the thread index (x,y,x) has thread ID=x+yDx+zDxDy, for block size (Dx,Dy,Dz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**When a kernel is started, the number of blocks per grid and the number of threads per block are fixed (gridDim and blockDim)**. CUDA makes four pieces of information available to each thread:\n",
    "- The thread index (threadIdx)\n",
    "- The block index (blockIdx)\n",
    "- The size and shape of a block (blockDim)\n",
    "- The size and shape of a grid (gridDim)\n",
    "\n",
    "Typically, each thread in a kernel will compute one element of an array. There is a common pattern to do this that most CUDA programs use are shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### CUDA simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you do not have a CUDA-enabled GPU on your system, \n",
    "# you will receive one of the following errors:\n",
    "\n",
    "# numba.cuda.cudadrv.error.CudaDriverError: CUDA initialized before forking\n",
    "# CudaSupportError: Error at driver init: \n",
    "# [3] Call to cuInit results in CUDA_ERROR_NOT_INITIALIZED:\n",
    "# numba.cuda.cudadrv.error.CudaDriverError: Error at driver init:\n",
    "# CUDA disabled by user:\n",
    "# If you do have a CUDA-enabled GPU on your system, you should see a message like:\n",
    "\n",
    "# <Managed Device 0>\n",
    "# If your machine has multiple GPUs, you might want to select which one to use. \n",
    "# By default the CUDA driver selects the fastest GPU as the device 0, \n",
    "# which is the default device used by Numba.\n",
    "\n",
    "# numba.cuda.select_device( device_id )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'export' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Using the CUDA simulator\n",
    "# If you don’t have a CUDA-enabled GPU \n",
    "# (i.e. you received one of the error messages described previously), \n",
    "# then you will need to use the CUDA simulator. \n",
    "# The simulator is enabled by setting the environment variable \n",
    "# NUMBA_ENABLE_CUDASIM to 1.\n",
    "\n",
    "\n",
    "# Mac/Linux\n",
    "# Launch a terminal shell and type the commands:\n",
    "!export NUMBA_ENABLE_CUDASIM=1\n",
    "\n",
    "# Windows\n",
    "# Launch a CMD shell and type the commands:\n",
    "# SET NUMBA_ENABLE_CUDASIM=1\n",
    "\n",
    "## export sets the value f na enivorment varlibe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: NUMBA_ENABLE_CUDASIM=1 ## could also run this on the notebook if you change enviorment vairbles here,\n"
     ]
    }
   ],
   "source": [
    "%env NUMBA_ENABLE_CUDASIM=1 ## could also run this on the notebook if you change enviorment vairbles here, \n",
    "# ! pip install numba\n",
    "# !conda install cudatoolkit\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\buzga\\anaconda3\\envs\\advpy\\lib\\site-packages\\numba\\core\\config.py:154: RuntimeWarning: environ NUMBA_ENABLE_CUDASIM defined but failed to parse '1 ## could also run this on the notebook if you change enviorment vairbles here,'\n",
      "  warnings.warn(\"environ %s defined but failed to parse '%s'\" %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Managed Device 0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<weakproxy at 0x000001D277C63DB0 to Device at 0x000001D277AB8910>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numba import cuda\n",
    "print(cuda.gpus)\n",
    "\n",
    "cuda.select_device(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\buzga\\anaconda3\\envs\\advpy\\lib\\site-packages\\numba\\cuda\\dispatcher.py:488: NumbaPerformanceWarning: \u001b[1mGrid size 16 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "data:\n",
      " [   0.   10.   20.   30.   40.   50.   60.   70.   80.   90.  100.  110.\n",
      "  120.  130.  140.  150.  160.  170.  180.  190.  200.  210.  220.  230.\n",
      "  240.  250.  260.  270.  280.  290.  300.  310.  320.  330.  340.  350.\n",
      "  360.  370.  380.  390.  400.  410.  420.  430.  440.  450.  460.  470.\n",
      "  480.  490.  500.  510.  520.  530.  540.  550.  560.  570.  580.  590.\n",
      "  600.  610.  620.  630.  640.  650.  660.  670.  680.  690.  700.  710.\n",
      "  720.  730.  740.  750.  760.  770.  780.  790.  800.  810.  820.  830.\n",
      "  840.  850.  860.  870.  880.  890.  900.  910.  920.  930.  940.  950.\n",
      "  960.  970.  980.  990. 1000. 1010. 1020. 1030. 1040. 1050. 1060. 1070.\n",
      " 1080. 1090. 1100. 1110. 1120. 1130. 1140. 1150. 1160. 1170. 1180. 1190.\n",
      " 1200. 1210. 1220. 1230. 1240. 1250. 1260. 1270. 1280. 1290. 1300. 1310.\n",
      " 1320. 1330. 1340. 1350. 1360. 1370. 1380. 1390. 1400. 1410. 1420. 1430.\n",
      " 1440. 1450. 1460. 1470. 1480. 1490. 1500. 1510. 1520. 1530. 1540. 1550.\n",
      " 1560. 1570. 1580. 1590. 1600. 1610. 1620. 1630. 1640. 1650. 1660. 1670.\n",
      " 1680. 1690. 1700. 1710. 1720. 1730. 1740. 1750. 1760. 1770. 1780. 1790.\n",
      " 1800. 1810. 1820. 1830. 1840. 1850. 1860. 1870. 1880. 1890. 1900. 1910.\n",
      " 1920. 1930. 1940. 1950. 1960. 1970. 1980. 1990. 2000. 2010. 2020. 2030.\n",
      " 2040. 2050. 2060. 2070. 2080. 2090. 2100. 2110. 2120. 2130. 2140. 2150.\n",
      " 2160. 2170. 2180. 2190. 2200. 2210. 2220. 2230. 2240. 2250. 2260. 2270.\n",
      " 2280. 2290. 2300. 2310. 2320. 2330. 2340. 2350. 2360. 2370. 2380. 2390.\n",
      " 2400. 2410. 2420. 2430. 2440. 2450. 2460. 2470. 2480. 2490. 2500. 2510.\n",
      " 2520. 2530. 2540. 2550.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\buzga\\anaconda3\\envs\\advpy\\lib\\site-packages\\numba\\cuda\\cudadrv\\devicearray.py:885: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "# %%writefile cuda01.py\n",
    "\n",
    "from __future__ import division\n",
    "from numba import cuda\n",
    "import numpy\n",
    "import math\n",
    "\n",
    "# CUDA kernel\n",
    "@cuda.jit\n",
    "def my_kernel(io_array):\n",
    "    \n",
    "    tx = cuda.threadIdx.x\n",
    "    bx = cuda.blockIdx.x\n",
    "    bw = cuda.blockDim.x\n",
    "    print(\"tx\")\n",
    "    print(\"bx\")\n",
    "    print(\"bw\")\n",
    "    \n",
    "    index = tx + bx * bw\n",
    "    io_array[index] = index * 10\n",
    "    print(\"i, t, b, w:\", index, tx, bx, bw)\n",
    "        \n",
    "        \n",
    "# Host code   \n",
    "cuda.select_device(0)\n",
    "data = numpy.ones(256)\n",
    "threadsperblock = 16 \n",
    "blockspergrid = math.ceil(data.shape[0] / threadsperblock)\n",
    "\n",
    "my_kernel[blockspergrid, threadsperblock](data)\n",
    "print(\"\\ndata:\\n\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### For a 2-dimensional grid:\n",
    "# tx = cuda.threadIdx.x\n",
    "# ty = cuda.threadIdx.y\n",
    "# bx = cuda.blockIdx.x\n",
    "# by = cuda.blockIdx.y\n",
    "# bw = cuda.blockDim.x\n",
    "# bh = cuda.blockDim.y\n",
    "# x = tx + bx * bw\n",
    "# y = ty + by * bh\n",
    "# array[x, y] = compute(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Hierarchy and Data Transfer\n",
    "The CPU and GPU have separate memory spaces. This means that data that is processed by the GPU must be moved from the CPU to the GPU before the computation starts, and the results of the computation must be moved back to the CPU once processing has completed.\n",
    "\n",
    "#### Global memory\n",
    "This memory is accessible to __all threads__ as well as the host (CPU).\n",
    "- Global memory is allocated and deallocated by the host\n",
    "- Used to initialize the data that the GPU will work on\n",
    "\n",
    "#### Shared memory\n",
    "__Each thread block__ has its own shared memory\n",
    "- Accessible only by threads within the block\n",
    "- Much faster than local or global memory\n",
    "- Requires special handling to get maximum performance\n",
    "- Only exists for the lifetime of the block\n",
    "\n",
    "#### Local memory\n",
    "__Each thread__ has its own private local memory\n",
    "- Only exists for the lifetime of the thread\n",
    "- Generally handled automatically by the compiler\n",
    "\n",
    "#### Constant and texture memory\n",
    "These are __read-only memory__ spaces accessible by __all threads__.\n",
    "- Constant memory is used to cache values that are shared by all functional units\n",
    "- Texture memory is optimized for texturing operations provided by the hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## OpenCL and pyOpenCL\n",
    "__OpenCL__ (Open Computing Language) is an **open standard** for cross-platform, **parallel programming**. It was originally developed by Apple in 2008 and is now maintained by the Khronos Group.\n",
    "\n",
    "<img src=\"opencl.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    " \n",
    "While OpenCL supports many different types of processors, as for example GPUs, DSPs, and FPGAs, it is most notably used to access the GPU for general computing tasks.\n",
    "\n",
    "__pyOpenCL__ is a package (MIT license) that enables developers to easily access the OpenCL API from Python.\n",
    "\n",
    "A standard and a minimal OpenCL code will have following parts.\n",
    "1. Identifying a Platform\n",
    "2. Finding the device ID\n",
    "3. Creating the context: _to manage objects such as command-queues, memory, program and kernel objects and for executing kernels on one or more devices specified in the context._\n",
    "4. Creating a command queue in the context\n",
    "5. Creating a program source and a kernel entry point\n",
    "6. Creating the buffers for data handling\n",
    "7. Kernel Program\n",
    "8. Build and Launch the Kernel\n",
    "9. Read the Output Buffer and clear it (if needed)\n",
    "\n",
    "A _pyopencl_ user will have its own device identified by environment variables, simplifying things. Examples can be found [here](https://github.com/inducer/pyopencl/tree/master/examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Important:**\n",
    "\n",
    "See and install pocl to get OpenCL device drivers: https://anaconda.org/conda-forge/pocl\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Platform - Name:  NVIDIA CUDA\n",
      "Platform - Vendor:  NVIDIA Corporation\n",
      "Platform - Version:  OpenCL 3.0 CUDA 11.6.134\n",
      "Platform - Profile:  FULL_PROFILE\n",
      "    ------\n",
      "    Device - Name:  NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "    Device - Type:  ALL | GPU\n",
      "    Device - Max Clock Speed:  1282 Mhz\n",
      "    Device - Compute Units:  30\n",
      "    Device - Local Memory:  48 KB\n",
      "    Device - Constant Memory:  64 KB\n",
      "    Device - Global Memory: 6 GB\n",
      "==========\n",
      "Platform - Name:  AMD Accelerated Parallel Processing\n",
      "Platform - Vendor:  Advanced Micro Devices, Inc.\n",
      "Platform - Version:  OpenCL 2.1 AMD-APP (3302.6)\n",
      "Platform - Profile:  FULL_PROFILE\n",
      "    ------\n",
      "    Device - Name:  gfx90c\n",
      "    Device - Type:  ALL | GPU\n",
      "    Device - Max Clock Speed:  2000 Mhz\n",
      "    Device - Compute Units:  8\n",
      "    Device - Local Memory:  32 KB\n",
      "    Device - Constant Memory:  4968461 KB\n",
      "    Device - Global Memory: 6 GB\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%writefile info.py\n",
    "\n",
    "# Find out about your computer's OpenCL situation\n",
    "import pyopencl as cl  # Import the OpenCL GPU computing API\n",
    "\n",
    "for platform in cl.get_platforms():  # Print each platfor m on this computer\n",
    "    print('=' * 10)\n",
    "    print('Platform - Name:  ' + platform.name)\n",
    "    print('Platform - Vendor:  ' + platform.vendor)\n",
    "    print('Platform - Version:  ' + platform.version)\n",
    "    print('Platform - Profile:  ' + platform.profile)\n",
    "    \n",
    "    for device in platform.get_devices():  # Print each device per-platform\n",
    "        print('    ' + '-' * 6)\n",
    "        print('    Device - Name:  ' + device.name)\n",
    "        print('    Device - Type:  ' + cl.device_type.to_string(device.type))\n",
    "        print('    Device - Max Clock Speed:  {0} Mhz'.format(device.max_clock_frequency))\n",
    "        print('    Device - Compute Units:  {0}'.format(device.max_compute_units))\n",
    "        print('    Device - Local Memory:  {0:.0f} KB'.format(device.local_mem_size/1024))\n",
    "        print('    Device - Constant Memory:  {0:.0f} KB'.format(device.max_constant_buffer_size/1024))\n",
    "        print('    Device - Global Memory: {0:.0f} GB'.format(device.global_mem_size/1073741824.0))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyopencl.Context at 0x1d276e02af0 on <pyopencl.Device 'NVIDIA GeForce RTX 3060 Laptop GPU' on 'NVIDIA CUDA' at 0x1d207619060>>\n",
      "Took 0.6550285816192627s\n",
      "a: [0.9246222  0.0188136  0.70861846 ... 0.9235955  0.9211652  0.91608006]\n",
      "b: [0.19859873 0.6600365  0.46249261 ... 0.02425529 0.1299972  0.33553386]\n",
      "c: [1.1232209  0.6788501  1.1711111  ... 0.94785076 1.0511625  1.2516139 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use OpenCL To Add Two Random Arrays (This Way Hides Details)\n",
    "from time import time\n",
    "import pyopencl as cl  # Import the OpenCL GPU computing API\n",
    "import pyopencl.array as pycl_array  # Import PyOpenCL Array \n",
    "#(a Numpy array plus an OpenCL buffer object)\n",
    "\n",
    "import numpy as np  # Import Numpy number tools\n",
    "\n",
    "# platform = cl.get_platforms()[0]  # Select the first platform [0]\n",
    "# device = platform.get_devices()[0]  # Select the first device on this platform [0]\n",
    "# context = cl.Context([device])  # Create a context with your device\n",
    "context = cl.create_some_context()  # Initialize the Context\n",
    "\n",
    "print(context)\n",
    "queue   = cl.CommandQueue(context)  # Instantiate a Queue\n",
    "\n",
    "# Create two random pyopencl arrays\n",
    "a = pycl_array.to_device(queue, np.random.rand(50000).astype(np.float32))\n",
    "b = pycl_array.to_device(queue, np.random.rand(50000).astype(np.float32))  \n",
    "\n",
    "# Create an empty pyopencl destination array\n",
    "ts = time()\n",
    "res_c = pycl_array.empty_like(a)  \n",
    "\n",
    "program = cl.Program(context, \"\"\"\n",
    "__kernel void sum(__global const float *a, __global const float *b, __global float *c)\n",
    "{\n",
    "  int i = get_global_id(0);\n",
    "  c[i] = a[i] + b[i];\n",
    "}\"\"\").build()  # Create the OpenCL program\n",
    "\n",
    "# Enqueue the program for execution and store the result in c\n",
    "program.sum(queue, a.shape, None, a.data, b.data, res_c.data)  \n",
    "print('Took {}s'.format(time() - ts))\n",
    "\n",
    "print(\"a: {}\".format(a))\n",
    "print(\"b: {}\".format(b))\n",
    "print(\"c: {}\".format(res_c))  \n",
    "# Print all three arrays, to show sum() worked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Example at PyOpenCL's documentation: https://documen.tician.de/pyopencl/\n",
    "\n",
    "\n",
    "See [https://documen.tician.de/pyopencl/runtime_program.html](https://documen.tician.de/pyopencl/runtime_program.html)\n",
    "\n",
    "and *associated memory object* `mem_info`: \n",
    "[https://documen.tician.de/pyopencl/runtime_const.html#mem_info](https://documen.tician.de/pyopencl/runtime_const.html#mem_info)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pyopencl.Device 'NVIDIA GeForce RTX 3060 Laptop GPU' on 'NVIDIA CUDA' at 0x1d207619060>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "platform.get_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devices are  [<pyopencl.Device 'NVIDIA GeForce RTX 3060 Laptop GPU' on 'NVIDIA CUDA' at 0x1d207619060>]\n",
      "<pyopencl.Context at 0x1d21b5df330 on <pyopencl.Device 'NVIDIA GeForce RTX 3060 Laptop GPU' on 'NVIDIA CUDA' at 0x1d207619060>>\n",
      "Took 0.08995461463928223s\n",
      "[0.5482081  0.991959   0.0330935  0.00536875 0.6443573  0.4908051\n",
      " 0.8984595  0.28630453 0.39605775 0.4997829 ]\n",
      "[0.6374001  0.6473233  0.63757753 0.5035525  0.62957776 0.184533\n",
      " 0.9711132  0.48127133 0.44080186 0.7624876 ]\n",
      "[1.1856081  1.6392822  0.67067105 0.50892127 1.2739351  0.6753381\n",
      " 1.8695726  0.76757586 0.8368596  1.2622705 ]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# the same above algorithm but written in a different way\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pyopencl as cl\n",
    "\n",
    "n = 5_000_000\n",
    "\n",
    "a_np = np.random.rand(n).astype(np.float32)\n",
    "b_np = np.random.rand(n).astype(np.float32)\n",
    "\n",
    "# ctx = cl.create_some_context()\n",
    "platform = cl.get_platforms()[0]  # Select the first platform [0]\n",
    "device = platform.get_devices()[0]  # Select the first device on this platform [0]\n",
    "print(\"devices are \",  platform.get_devices())\n",
    "ctx = cl.Context([device])  # Create a context with your device\n",
    "print(ctx)\n",
    "queue = cl.CommandQueue(ctx)\n",
    "\n",
    "# Buffer: class pyopencl.Buffer(context, flags, size=0, hostbuf=None)\n",
    "\n",
    "mf = cl.mem_flags\n",
    "a_g = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=a_np)\n",
    "b_g = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=b_np)\n",
    "\n",
    "\n",
    "# get_global_id\n",
    "# Returns the unique global work-item ID value \n",
    "# for dimension identified by dimindx.\n",
    "\n",
    "prg = cl.Program(ctx, \"\"\"\n",
    "__kernel void sum(\n",
    "    __global const float *a_g, __global const float *b_g, __global float *res_g)\n",
    "{\n",
    "  int gid = get_global_id(0);\n",
    "  res_g[gid] = a_g[gid] + b_g[gid];\n",
    "}\n",
    "\"\"\").build()\n",
    "\n",
    "ts = time()\n",
    "\n",
    "res_g = cl.Buffer(ctx, mf.WRITE_ONLY, a_np.nbytes)\n",
    "prg.sum(queue, a_np.shape, None, a_g, b_g, res_g)\n",
    "\n",
    "res_np = np.empty_like(a_np)\n",
    "cl.enqueue_copy(queue, res_np, res_g)\n",
    "\n",
    "print('Took {}s'.format(time() - ts))\n",
    "\n",
    "# Check on CPU with Numpy:\n",
    "print(a_np[0: 10])\n",
    "print(b_np[0: 10])\n",
    "print(res_np[0: 10])\n",
    "\n",
    "print((res_np - (a_np + b_np))[0:10])\n",
    "print(np.linalg.norm(res_np - (a_np + b_np)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "pyOpenCL has two goals:\n",
    "- Make OpenCL seem simple\n",
    "- Expose OpenCL's complex features\n",
    "\n",
    "Comparing the two previous codes we see that\n",
    "```python\n",
    "context = cl.create_some_context()\n",
    "```\n",
    "is simple, but if you have two GPUs in your computer, this function might select the wrong one.  Therefore, you might want to write three lines instead of one:\n",
    "```python\n",
    "platform = cl.get_platforms()[0]  # Select the first platform [0]\n",
    "device = platform.get_devices()[0]  # Select the first device on this platform [0]\n",
    "context = cl.Context([device])  # Create a context with your device\n",
    "```\n",
    "This second way of creating a context is longer, but it allows you to target the exact platform and device you want to use on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
